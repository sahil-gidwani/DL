# -*- coding: utf-8 -*-
"""RNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QE8IFWar5ZZsNGxNVh-5BoJpx9MJZxw_
"""

# 1. Importing Libraries

import math
import numpy as np
import seaborn as sns
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout
import seaborn as sns
import matplotlib.pyplot as plt

# 2. Reading the dataset

df = pd.read_csv("https://raw.githubusercontent.com/sahil-gidwani/DL/main/data/GOOGL.csv")
df

# 3. EDA & Preproccesing

# Shape of the dataset (Rows, Columns)
df.shape

# Dropping 'symble' column
# df.drop(columns =['symbol'] , inplace=True)

# Searching for missing values
df.isna().sum()

df['Date'] = pd.to_datetime(df['Date'], utc=True)
df

df.info()

# 4. Visualization

sns.lineplot(data=df, x='Date', y='Open')

# 5. Prepering data for trainning and testing the model

DF = df.copy()
DF['Date'] = pd.to_datetime(DF['Date'])
# Set 'Date' column as index
DF = DF.set_index('Date')
DF

training_set = DF[:'2020'].iloc[:,0:1].values
test_set = DF['2020':].iloc[:,0:1].values

# Normalization is very important in all deep learning in general. Normalization makes the properties more consistent.
# Scaling the training set
sc = MinMaxScaler(feature_range=(0,1))
training_set_scaled = sc.fit_transform(training_set)

timesteps = 60

"""
Since LSTMs store long term memory state, we create a data structure with 60 timesteps and 1 output.
For each element of training set, we have 60 previous training set elements.

"""

X_train = []
y_train = []
for i in range(timesteps,1147):
    X_train.append(training_set_scaled[i-timesteps:i,0])
    y_train.append(training_set_scaled[i,0])
X_train, y_train = np.array(X_train), np.array(y_train)

# Reshaping X_train for efficient modelling
X_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))

"""
We get the test set ready in a similar way as the training set.
The following has been done so first 60 entires of test set have 60 previous values
which is impossible to get unless we take the whole 'High' attribute data for processing

"""

dataset_total = pd.concat((DF['Close'][:'2020'], DF['Close']['2020':]),axis=0)
inputs = dataset_total[len(dataset_total)-len(test_set) - 60:].values
inputs = inputs.reshape(-1,1)
inputs  = sc.transform(inputs)

# Preparing X_test
X_test = []
for i in range(timesteps, len(inputs)):
    X_test.append(inputs[i-timesteps:i,0])
X_test = np.array(X_test)
X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))

# 6. Creating the model

# Define the LSTM architecture
Model = Sequential()

# First LSTM layer with Dropout regularization
Model.add(LSTM(units=100, return_sequences=True, input_shape=(X_train.shape[1], 1)))
Model.add(Dropout(0.2))

# Second LSTM layer with Dropout regularization
Model.add(LSTM(units=100, return_sequences=True))
Model.add(Dropout(0.2))

# Third LSTM layer with Dropout regularization
Model.add(LSTM(units=100, return_sequences=True))
Model.add(Dropout(0.2))

# Fourth LSTM layer without return_sequences since it's the last LSTM layer
Model.add(LSTM(units=100))
Model.add(Dropout(0.2))

# Fully connected layers
Model.add(Dense(units=25))
Model.add(Dense(units=1))  # Output layer

# Summary of the model architecture
Model.summary()

# Compiling the model
Model.compile(optimizer= 'adam', loss = 'mean_squared_error', metrics =['accuracy'])

# Epochs and Batch Size
epochs = 15
batch_size = 32

#from keras import callbacks
#earlystopping = callbacks.EarlyStopping(monitor ="val_loss", mode ="min", patience = 2, restore_best_weights = True)

# Fitting the model
history =  Model.fit(X_train, y_train, epochs = epochs, batch_size = batch_size)

loss = history.history['loss']

epochs = range(len(loss))

plt.plot(epochs, loss, 'r', label='Training loss')

plt.title('Training loss', size=15, weight='bold')
plt.legend(loc=0)
plt.figure()

plt.show()

predicted_stock_price = Model.predict(X_test)
predicted_stock_price = sc.inverse_transform(predicted_stock_price)

# Some functions to help out with
def plot_predictions(test,predicted):
    plt.plot(test, color='red',label='Real Google Stock Price')
    plt.plot(predicted, color='blue',label='Predicted Google Stock Price')
    plt.title('Google Stock Price Prediction')
    plt.xlabel('Time')
    plt.ylabel('Google Stock Price')
    plt.legend()
    plt.show()

# Visualizing the results for LSTM
plot_predictions(test_set, predicted_stock_price)

